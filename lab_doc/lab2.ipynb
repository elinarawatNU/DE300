{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0460247-9881-47e2-a04a-585e6824f223",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "\n",
    "## 0, SSH to your EC2 instance\n",
    "Start your instance and run:\n",
    "\n",
    "```\n",
    "ssh -i YOUR_KEY_PATH -L 8888:localhost:8888 EC2_IP_ADDRESS\n",
    "```\n",
    "Note that we add the option '-L 8888:localhost:8888' to create a port tunnel between EC2 8888 and your local machine 8888.\n",
    "\n",
    "\n",
    "## 1, Install Required Tools/Packages\n",
    "    \n",
    "(1)Install the Tmux on your EC2 instance, see https://github.com/tmux/tmux/wiki/Installing. See Tmux cheetsheet https://tmuxcheatsheet.com/. Important functions includes start new sessions, attach/detach sessions, create/delete new windows in a session, navigate between sessions/windows.  (Though the Tmux is optional, this tool will give you a lot convenience when navigating between multiple shells)\n",
    "\n",
    "(2)Install the AWS CLI (AWS command line interface) on your EC2 instance. See https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html. (You can also install this on your local laptop, in case you want to upload/download between your local laption and EC2/S3 bucket\n",
    "\n",
    "(3) Configure your AWS CLI, following instructions at the page: https://www.cloud.northwestern.edu/resources/howtos/use-netid-authentication-with-the-aws-cli/ . This includes running \n",
    "```\n",
    "aws configure sso --no-browser\n",
    "```\n",
    "and set your credentials (available at your login page of AWS at 'access key') by running \n",
    "```\n",
    "export AWS_ACCESS_KEY_ID=\n",
    "export AWS_SECRET_ACCESS_KE=\"\n",
    "export AWS_SESSION_TO==\"\n",
    "\n",
    "```\n",
    "\n",
    "(4) Install the Docker on your EC2 instance by running \n",
    "```\n",
    "sudo apt install docker.io\n",
    "```\n",
    "and set permissions\n",
    "```\n",
    "sudo chmod 666 /var/run/docker.sock    \n",
    "```\n",
    "\n",
    "\n",
    "## 2, Github Set Up\n",
    "\n",
    " - generate SSH key: https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\n",
    " - add the public key to your Github: https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account\n",
    "\n",
    "   After set up the key pairs, please create a course repo (e.g yourGithubAccount/DE300) on the Github webpage, 'git pull' your repo to certain directory at your instance, create a test file by 'touch test', then run\n",
    "   ```\n",
    "    git add .\n",
    "    git commit -m \"Initial commit\"\n",
    "    git remote add origin <repo URL i.e https://github.com/(your Github name)/repo name.git>\n",
    "    git push origin main\n",
    "   ```\n",
    "If your test file appear on your remote Github repo, then you have successfully build connection between your local repo and your remote repo!\n",
    "\n",
    " \n",
    "## 3 Create your first Docker(Jupyter Notebook) Image and run the Docker Container with this image\n",
    "(1) Under your course directory, create a folder my-jupyter-image (you can choose your preferred name). In this folder, create file with name Dockerfile that contains the following txt:\n",
    "\n",
    "```\n",
    "# Use the Jupyter Data Science Notebook as the base image\n",
    "FROM jupyter/datascience-notebook\n",
    "\n",
    "# Set the working directory to /home/jovyan - the default for Jupyter images\n",
    "WORKDIR /home/jovyan\n",
    "\n",
    "EXPOSE 8888\n",
    "\n",
    "# Avoid prompts from apt during the build process\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "COPY . .\n",
    "\n",
    "# Install the latest AWS CLI version 2\n",
    "USER root\n",
    "\n",
    "# Update and install necessary packages\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    wget \\\n",
    "    curl \\\n",
    "    unzip \\\n",
    "    bzip2 \\\n",
    "    git \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Install the AWS CLI tools to your image\n",
    "RUN curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" && \\\n",
    "    unzip awscliv2.zip && \\\n",
    "    ./aws/install && \\\n",
    "    rm -rf awscliv2.zip ./aws\n",
    "USER jovyan\n",
    "\n",
    "\n",
    "# Start the Jupyter Notebook server when the container launches\n",
    "# The base image already configures Jupyter to run on start, but you can customize\n",
    "# startup options here if needed.\n",
    "CMD [\"start-notebook.sh\", \"--NotebookApp.token=''\", \"--NotebookApp.password=''\", \"--allow-root\"]\n",
    "\n",
    "```\n",
    "The 'Dockerfile' is an instruction to create the docker image\n",
    "\n",
    "(2) Create your image, cd to the myjupyter folder, run \n",
    "```\n",
    "docker build -t my_jupyter .\n",
    "```\n",
    "After running the above command, you can check the image is created by running 'docker images'. Building the myjupyter image first time will take some time, since this image is built from the base image 'jupyter/datascience-notebook', which is about 6GB and it takes time to download from the 'docker hub' (this is like github repo to git, search this!). So when you 'docker images', you can actuallly see two images, one is 'jupyter/datascience-notebook' as the base image, the other is the one you created with the name 'myjupyter'.\n",
    "\n",
    "(3) Create your SAMPLE_PROJ_PATH. Runte the container in your EC2 instance wit 'myjupytr' image:\n",
    "```  \n",
    "docker run -p 8888:8888 -v SAMPLE_PROJ_PATH:/home/jovyan/ myjupyt \n",
    "\n",
    "```\n",
    "\n",
    "In the above command, the option -p sets up the port tunnel: the container's 8888 tunnel, which the jupyter notebook application occupies, sync with your instance 8888 port. The option '-v SAMPLE_PROJ_PATH:/home/jovyan/' tells the container to sync your working directory 'SAMPLE_PROJ_PATH'.\n",
    "\n",
    "You can check your container by command 'docker ps -a', which will show all your containers. Option '-a' makes it show both running and stopped containers.\n",
    "\n",
    "Search the following command and see what they do:\n",
    "```\n",
    "docker start container_id\n",
    "docker stop container_id\n",
    "docker logs container_id\n",
    "\n",
    "docker rm container_id\n",
    "docker rmi image_id\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ef98e7-f2a7-46d0-8002-2116aacac090",
   "metadata": {},
   "source": [
    "## Lab2 Assigment\n",
    "\n",
    "(1) Find the bucket 'de300spring2024' in the S3 buckets, create your own folder with your name. This will be the volume that you will use to store all your data in this course. Please do not modify files in with other students' folders.\n",
    "\n",
    "(2) Use 'aws s3 cp s3://de300spring2024/robert_su/sample_dataset.csv YOUR_PATH' to copy the file 'sample_dataset.csv' from TA(Robert Su) folder to your folder\n",
    "\n",
    "(3) You can open the jupyter lab visiting 'http://localhost:8888/lab/' in your local machine browser. (This lab/notebook is running in the container in your EC2)\n",
    "\n",
    "(4) Create a jupyter notebook notebook with name 'reading_data.ipynb' can run the following python code cells, if you can read the data, then it means your set up is good to go!\n",
    "\n",
    "(5) After doing all this, push your EC2 git repo to the remote main repo, the mentors will grade based on your remote repo origin/main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa97cb4-c4e5-43e8-a7ed-11f1e0b478de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading required package\n",
    "\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00fdeca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to change the credentials for yourself\n",
    "\n",
    "s3 = boto3.client('s3',\n",
    "                  aws_access_key_id='...',\n",
    "                  aws_secret_access_key='...',\n",
    "                  aws_session_token='...')\n",
    "\n",
    "\n",
    "bucket_name = 'de300spring2024'\n",
    "object_key = 'robert_su/sample_dataset.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48557fbb-c465-47d4-999a-e2d1fdbc593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_obj = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad2cde7a-96f9-448a-838a-d09fa2a6b284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID           Name  Age         City\n",
      "0   1       John Doe   28     New York\n",
      "1   2     Jane Smith   32  Los Angeles\n",
      "2   3    Emily Davis   45      Chicago\n",
      "3   4  Michael Brown   22        Miami\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(BytesIO(csv_string.encode()))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c5377-9b5d-4803-966a-4d51c08af2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
