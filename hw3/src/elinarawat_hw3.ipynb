{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, FloatType, LongType, StringType, DoubleType\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col, when , isnan, count, udf\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, Imputer\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, LinearSVC, DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator, Evaluator\n",
    "import pyspark.sql.functions as F\n",
    "from itertools import combinations\n",
    "import os\n",
    "\n",
    "# import requests\n",
    "# import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(spark: SparkSession) -> DataFrame:\n",
    "    \"\"\"\n",
    "    read data; since the data has the header we let spark guess the schema\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(os.path.join(DATA_FOLDER, \"heart_disease.csv\"))\n",
    "    print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df: DataFrame):\n",
    "    columns = ['age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', 'smoke', 'fbs', 'prop', 'nitr', 'pro', 'diuretic', 'thaldur', 'thalach', 'exang', 'oldpeak', 'slope', 'target']\n",
    "    df = df.select(columns)\n",
    "    df = df.dropna(subset=[\"target\"])\n",
    "    df = df.withColumn(\"trestbps\", when(F.col(\"trestbps\") < 100, 100).otherwise(F.col(\"trestbps\")))\n",
    "    df = df.withColumn(\"oldpeak\", when(F.col(\"oldpeak\") < 0, 0).when(F.col(\"oldpeak\") > 4, 4).otherwise(F.col(\"oldpeak\")))\n",
    "    for col_name in [\"painloc\", \"painexer\", 'fbs', 'prop', 'nitr', 'pro', 'diuretic']:\n",
    "        df = df.withColumn(col_name, when(F.col(col_name) < 0, 0).when(F.col(col_name) > 1, 1).otherwise(F.col(col_name)))\n",
    "    df = df.withColumn(\"age\", df[\"age\"].cast(IntegerType()))\n",
    "    print(df)\n",
    "\n",
    "    # def fetch_html(url):\n",
    "    #     response = requests.get(url)\n",
    "    #     return response.text\n",
    "\n",
    "    # fetch_html_udf = udf(fetch_html, StringType())\n",
    "\n",
    "    # # Fetch HTML content for ABS and CDC data\n",
    "    # abs_url = \"https://www.abs.gov.au/statistics/health/health-conditions-and-risks/smoking-and-vaping/latest-release\"\n",
    "    # cdc_url = \"https://www.cdc.gov/tobacco/data_statistics/fact_sheets/adult_data/cig_smoking/index.htm\"\n",
    "\n",
    "    # abs_html = fetch_html(abs_url)\n",
    "    # cdc_html = fetch_html(cdc_url)\n",
    "\n",
    "    # # Function to parse the ABS data\n",
    "    # def parse_abs_data(html):\n",
    "    #     pattern = re.compile(r'<tr>(.*?)</tr>', re.DOTALL)\n",
    "    #     rows = pattern.findall(html)\n",
    "    #     data = []\n",
    "    #     for row in rows:\n",
    "    #         cols = re.findall(r'<td>(.*?)</td>', row)\n",
    "    #         if cols:\n",
    "    #             age_range = re.findall(r'\\d+', cols[0])\n",
    "    #             percentage = re.findall(r'\\d+\\.\\d+', cols[1])\n",
    "    #             if age_range and percentage:\n",
    "    #                 data.append({\"Age Range\": age_range, \"2022 (%)\": float(percentage[0])})\n",
    "    #     return data\n",
    "\n",
    "    # # Function to parse the CDC data\n",
    "    # def parse_cdc_data(html):\n",
    "    #     pattern = re.compile(r'<li>(.*?)</li>', re.DOTALL)\n",
    "    #     items = pattern.findall(html)\n",
    "    #     age_data = []\n",
    "    #     sex_data = []\n",
    "    #     for item in items:\n",
    "    #         if 'aged' in item:\n",
    "    #             age_range = re.findall(r'\\d+', item)\n",
    "    #             percentage = re.findall(r'\\d+\\.\\d+', item)\n",
    "    #             if age_range and percentage:\n",
    "    #                 age_data.append({\"Age Range\": age_range, \"2022 (%)\": float(percentage[0])})\n",
    "    #         if 'Male' in item or 'Female' in item:\n",
    "    #             sex = 'Male' if 'Male' in item else 'Female'\n",
    "    #             percentage = re.findall(r'\\d+\\.\\d+', item)\n",
    "    #             if percentage:\n",
    "    #                 sex_data.append({\"Sex\": sex, \"2022 (%)\": float(percentage[0])})\n",
    "    #     return age_data, sex_data\n",
    "\n",
    "    # # Parse the HTML content\n",
    "    # abs_data = parse_abs_data(abs_html)\n",
    "    # cdc_age_data, cdc_sex_data = parse_cdc_data(cdc_html)\n",
    "\n",
    "    # # Broadcast the data\n",
    "    # abs_data_broadcast = spark.sparkContext.broadcast(abs_data)\n",
    "    # cdc_age_data_broadcast = spark.sparkContext.broadcast(cdc_age_data)\n",
    "    # cdc_sex_data_broadcast = spark.sparkContext.broadcast(cdc_sex_data)\n",
    "\n",
    "\n",
    "    # def age_to_smoke_percent_udf(age):\n",
    "    #     abs_data = abs_data_broadcast.value\n",
    "    #     if age is None:\n",
    "    #         return None\n",
    "    #     for row in abs_data:\n",
    "    #         age_range = row[\"Age Range\"]\n",
    "    #         if len(age_range) == 2:\n",
    "    #             if int(age) >= int(age_range[0]) and int(age) <= int(age_range[1]):\n",
    "    #                 return float(row['2022 (%)'])\n",
    "    #         elif len(age_range) == 1:\n",
    "    #             if int(age) >= int(age_range[0]):\n",
    "    #                 return float(row['2022 (%)'])\n",
    "    #     return None\n",
    "\n",
    "    # def cdc_age_sex_smoke_udf(age, sex):\n",
    "    #     cdc_age_data = cdc_age_data_broadcast.value\n",
    "    #     cdc_sex_data = cdc_sex_data_broadcast.value\n",
    "    #     if age is None or sex is None:\n",
    "    #         return None\n",
    "    #     age_rate = None\n",
    "    #     for row in cdc_age_data:\n",
    "    #         age_range = row['Age Range']\n",
    "    #         if len(age_range) == 2:\n",
    "    #             if int(age) >= int(age_range[0]) and int(age) <= int(age_range[1]):\n",
    "    #                 age_rate = float(row['2022 (%)'])\n",
    "    #         elif len(age_range) == 1:\n",
    "    #             if int(age) >= int(age_range[0]):\n",
    "    #                 age_rate = float(row['2022 (%)'])\n",
    "    #     if sex == 0:\n",
    "    #         sex_rate = float(cdc_sex_data[0]['2022 (%)'])\n",
    "    #     else:\n",
    "    #         female_rate = float(cdc_sex_data[1]['2022 (%)'])\n",
    "    #         male_rate = float(cdc_sex_data[0]['2022 (%)'])\n",
    "    #         sex_rate = age_rate * (male_rate / female_rate)\n",
    "    #     return sex_rate\n",
    "\n",
    "    # def smoke_transform_udf(x):\n",
    "    #     if x is None:\n",
    "    #         return None\n",
    "    #     else:\n",
    "    #         x = float(x)\n",
    "    #         return 1 if x >= 12 else 0\n",
    "\n",
    "    # def impute_smoke_transform_udf(smoke, abs_transformed, cdc_transformed):\n",
    "    #     if smoke is None:\n",
    "    #         return 1 if abs_transformed == 1 or cdc_transformed == 1 else 0\n",
    "    #     return smoke\n",
    "\n",
    "    # # Register UDFs\n",
    "    # age_to_smoke_percent_udf = udf(age_to_smoke_percent_udf, DoubleType())\n",
    "    # cdc_age_sex_smoke_udf = udf(cdc_age_sex_smoke_udf, DoubleType())\n",
    "    # smoke_transform_udf = udf(smoke_transform_udf, DoubleType())\n",
    "    # impute_smoke_transform_udf = udf(impute_smoke_transform_udf, DoubleType())\n",
    "\n",
    "    # # Apply UDFs to DataFrame\n",
    "    # df = df.withColumn(\"abs_smoke\", age_to_smoke_percent_udf(F.col(\"age\")))\n",
    "    # df = df.withColumn(\"cdc_smoke\", cdc_age_sex_smoke_udf(F.col(\"age\"), F.col(\"sex\")))\n",
    "    # df = df.withColumn(\"abs_transformed\", smoke_transform_udf(F.col(\"abs_smoke\")))\n",
    "    # df = df.withColumn(\"cdc_transformed\", smoke_transform_udf(F.col(\"cdc_smoke\")))\n",
    "    # df = df.withColumn(\"smoke\", impute_smoke_transform_udf(F.col(\"smoke\"), F.col(\"abs_transformed\"), F.col(\"cdc_transformed\")))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3., 4., and 5.: Split Data into Train and Test, Train binary classification models on data, Select Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(df: DataFrame):\n",
    "\n",
    "    \"\"\"\n",
    "    every attribute that is numeric is non-categorical; this is questionable\n",
    "    \"\"\"\n",
    "\n",
    "    numeric_features = [f.name for f in df.schema.fields if isinstance(f.dataType, (DoubleType, FloatType, IntegerType, LongType))]\n",
    "   \n",
    "    numeric_features.remove(\"target\")\n",
    "\n",
    "    imputed_columns_numeric = [f\"Imputed{v}\" for v in numeric_features]\n",
    "    imputer_numeric = Imputer(inputCols=numeric_features, outputCols=imputed_columns_numeric, strategy=\"mean\")\n",
    "\n",
    "\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=imputed_columns_numeric, \n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "\n",
    "    rf = RandomForestClassifier(labelCol='target', featuresCol='features')\n",
    "    lr = LogisticRegression(labelCol='target', featuresCol='features', maxIter=1000)\n",
    "    svc = LinearSVC(labelCol='target', featuresCol='features')\n",
    "    dt = DecisionTreeClassifier(labelCol='target', featuresCol='features')\n",
    "    class_dict = {\"RandFor\": rf, \"LogReg\": lr, \"SVC\": svc, \"DecTree\": dt}\n",
    "\n",
    "    PG_rf = ParamGridBuilder().addGrid(rf.numTrees, [20, 50]).build()\n",
    "    PG_lr = ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.1]).build()\n",
    "    PG_svc = ParamGridBuilder().addGrid(svc.regParam, [0.01, 0.1]).build()\n",
    "    PG_dt = ParamGridBuilder().addGrid(dt.maxDepth, [4, 6, 8, 10, 12]).addGrid(dt.minInstancesPerNode, [1, 2, 4, 6]).build()\n",
    "\n",
    "    PG_dict = {\"RandFor\": PG_rf, \"LogReg\": PG_lr, \"SVC\": PG_svc, \"DecTree\": PG_dt}\n",
    "\n",
    "    evaluator_roc_auc = BinaryClassificationEvaluator(labelCol='target', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "    evaluator_f1 = MulticlassClassificationEvaluator(labelCol='target', predictionCol='prediction', metricName='f1')\n",
    "    evaluator_accuracy = MulticlassClassificationEvaluator(labelCol='target', predictionCol='prediction', metricName='accuracy')\n",
    "\n",
    "\n",
    "    train, test = df.randomSplit([0.9, 0.1], seed=42)\n",
    "\n",
    "    best_roc_auc = float(\"-inf\")\n",
    "    best_f1 = float(\"-inf\")\n",
    "    best_accuracy = float(\"-inf\")\n",
    "\n",
    "    full_scores_dict = {}\n",
    "    scores_dict = {}\n",
    "\n",
    "   \n",
    "\n",
    "    for ML_model, model_class in class_dict.items():\n",
    "        stages = [imputer_numeric, assembler, model_class]\n",
    "        pipeline = Pipeline(stages=stages)\n",
    "\n",
    "\n",
    "        crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=PG_dict[ML_model], evaluator = BinaryClassificationEvaluator(labelCol='target'), numFolds=5, seed=42)\n",
    "        cvModel = crossval.fit(train)\n",
    "        predictions = cvModel.transform(test)\n",
    "\n",
    "        roc_auc = evaluator_roc_auc.evaluate(predictions)\n",
    "        f1 = evaluator_f1.evaluate(predictions)\n",
    "        accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "\n",
    "        print(f\"{ML_model} ROC AUC: {roc_auc}, F1: {f1}, Accuracy: {accuracy}\")\n",
    "\n",
    "        score = roc_auc + f1 + accuracy\n",
    "\n",
    "        full_scores_dict[ML_model] = [roc_auc, f1, accuracy]\n",
    "        scores_dict[ML_model] = score\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    best_model = max(scores_dict, key=lambda x: scores_dict[x])\n",
    "\n",
    "    for model in scores_dict:\n",
    "        if full_scores_dict[model][0] > best_roc_auc:\n",
    "            best_roc_auc = full_scores_dict[model][0]\n",
    "        if full_scores_dict[model][1] > best_f1:\n",
    "            best_f1 = full_scores_dict[model][1]\n",
    "        if full_scores_dict[model][2] > best_accuracy:\n",
    "            best_accuracy = full_scores_dict[model][2]\n",
    "\n",
    "    print(f\"Best Model: {best_model}, Best ROC_AUC: {best_roc_auc}, Best F1: {best_f1}, Best Accuracy: {best_accuracy}\")\n",
    "\n",
    "    \n",
    "    final_dict = {\"RandFor\": \"RandomForest\", \"LogReg\": \"LogisticRegression\", \"SVC\": \"SVC\", \"DecTree\": \"DecisionTree\"}\n",
    "    print(f\"Final Selected Model: {final_dict[best_model]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Heart Disease\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    try:\n",
    "        df = read_data(spark)\n",
    "        df = data_cleaning(df)\n",
    "        \n",
    "        pipeline(df)\n",
    "    \n",
    "    finally:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 01:02:29 WARN Utils: Your hostname, Elinas-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 172.18.247.249 instead (on interface en0)\n",
      "24/06/07 01:02:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/07 01:02:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[age: string, sex: int, painloc: int, painexer: int, relrest: int, pncaden: string, cp: int, trestbps: int, htn: int, chol: int, smoke: int, cigs: int, years: int, fbs: int, dm: int, famhist: int, restecg: int, ekgmo: int, ekgday(day: int, ekgyr: int, dig: int, prop: int, nitr: int, pro: int, diuretic: int, proto: int, thaldur: double, thaltime: double, met: double, thalach: int, thalrest: int, tpeakbps: int, tpeakbpd: int, dummy: int, trestbpd: int, exang: int, xhypo: int, oldpeak: double, slope: int, rldv5: int, rldv5e: int, ca: int, restckm: string, exerckm: int, restef: double, restwm: int, exeref: double, exerwm: int, thal: int, thalsev: int, thalpul: int, earlobe: int, cmo: int, cday: int, cyr: int, target: int]\n",
      "DataFrame[age: int, sex: int, painloc: int, painexer: int, cp: int, trestbps: int, smoke: int, fbs: int, prop: int, nitr: int, pro: int, diuretic: int, thaldur: double, thalach: int, exang: int, oldpeak: double, slope: int, target: int]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 01:02:38 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandFor ROC AUC: 0.8837792642140471, F1: 0.7794238683127573, Accuracy: 0.7777777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/07 01:03:05 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/06/07 01:03:05 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg ROC AUC: 0.8879598662207357, F1: 0.8213256157076383, Accuracy: 0.8194444444444444\n",
      "SVC ROC AUC: 0.8913043478260869, F1: 0.8213256157076383, Accuracy: 0.8194444444444444\n",
      "DecTree ROC AUC: 0.8269230769230769, F1: 0.8333333333333334, Accuracy: 0.8333333333333334\n",
      "Best Model: SVC, Best ROC_AUC: 0.8913043478260869, Best F1: 0.8333333333333334, Best Accuracy: 0.8333333333333334\n",
      "Final Selected Model: SVC\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
